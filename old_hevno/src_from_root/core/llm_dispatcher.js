// src/core/llm_dispatcher.js

import * as geminiCaller from '../llm_callers/gemini_caller.js';
// æœªæ¥å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šcaller:
// import * as openaiCaller from '../llm_callers/openai_caller.js';

const providers = {
    'gemini': geminiCaller,
    // 'openai': openaiCaller,
};

/**
 * æ ¹æ®llmConfigä¸­çš„providerï¼Œå°†è¯·æ±‚åˆ†æ´¾ç»™æ­£ç¡®çš„LLMè°ƒç”¨è€…ã€‚
 * @param {string} prompt - æ¸²æŸ“åçš„promptã€‚
 * @param {object} llmConfig - æ¨¡å—çš„LLMé…ç½®ã€‚
 * @returns {Promise<string>} æ¥è‡ªLLMçš„å“åº”ã€‚
 */
export async function dispatch(prompt, llmConfig) {
    const providerName = llmConfig.provider;
    if (!providerName) {
        throw new Error("Module is missing 'provider' field in its 'llm' configuration.");
    }

    const caller = providers[providerName.toLowerCase()];
    if (!caller) {
        throw new Error(`Unsupported LLM provider: "${providerName}". No caller found.`);
    }

    // ç®€æ´çš„è°ƒåº¦æ—¥å¿—
    console.log(`[LLM-Dispatch] ğŸš€ Routing to ${providerName}/${llmConfig.model || 'default'}`);

    try {
        const result = await caller.execute(prompt, llmConfig);
        console.log(`[LLM-Dispatch] âœ… ${providerName} completed successfully`);
        return result;
    } catch (error) {
        console.error(`[LLM-Dispatch] âŒ ${providerName} failed: ${error.message}`);
        throw error;
    }
}